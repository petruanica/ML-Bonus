{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Optional: Scikit-learn primer.\n",
    "In this additional assignment, you will learn to use the scikit-learn library.   \n",
    "**Note:** It is highly recommended to go through this notebook before starting with the bonus assignment.\n",
    " $\\newcommand{\\q}[1]{\\rightarrow \\textbf{Question #1}}$\n",
    " $\\newcommand{\\ex}[1]{\\rightarrow \\textbf{Exercise #1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Introduction\n",
    "All algorithms, both for training models and for pre-processing data, in scikit-learn have been implemented with the same `fit`, `predict` and `transform` API. As soon as you have learned this API you can use any algorithm without having to implement it on your own. For a given learning problem, you can then apply all those algortihms in the same way. The API also hides all the complex optimization choices that have to be made. You can control these by changing the hyper-parameters. The effects of these choices have been well documented in the API documentation and the provided tutorials of scikit-learn.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this assignment, we will use the Iris dataset to keep things simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Using classifiers\n",
    "Using a classifier in scikit-learn consist of 3 steps:\n",
    "1. Initialize the model. During this step, you can already give it some default hyper-parameters.\n",
    "2. Fitting the model on the training data.\n",
    "3. Making predictions and/or evaluating the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create\n",
    "Creating models is very easy in scikit-learn. All you have to do is create a new instance of the model's class.\n",
    "\n",
    "$\\ex{1}$ Extend the list of models with the `SVC` and `LogisticRegression` algorithms. Give the SVM a `poly` kernel. Give the Logistic Regression a maximum number of iterations of `1000`. Also, give both algorithms a regularization constant `C=0.5` and `random_state=42`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "models = {\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "    \"DummyClassifier\": DummyClassifier(strategy=\"most_frequent\"),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(max_depth=None, min_samples_leaf=2, random_state=random_state),\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(n_neighbors=3, weights=\"distance\"),\n",
    "    # START ANSWER\n",
    "    \"SVM\": SVC(kernel=\"poly\", C=0.5, random_state=42),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000, C=0.5, random_state=42),\n",
    "    # END ANSWER\n",
    "}\n",
    "\n",
    "assert \"GaussianNB\" in models and isinstance(models[\"GaussianNB\"], GaussianNB), \"There is no GaussianNB in models\"\n",
    "assert \"DecisionTreeClassifier\" in models and isinstance(models[\"DecisionTreeClassifier\"], DecisionTreeClassifier), \"There is no DecisionTreeClassifier in models\"\n",
    "assert \"KNeighborsClassifier\" in models and isinstance(models[\"KNeighborsClassifier\"], KNeighborsClassifier), \"There is no KNeighborsClassifier in models\"\n",
    "assert \"SVM\" in models and isinstance(models[\"SVM\"], SVC), \"There is no SVC in models\"\n",
    "assert \"LogisticRegression\" in models and isinstance(models[\"LogisticRegression\"], LogisticRegression), \"There is no LogisticRegression in models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Fit\n",
    "$ \\ex{2} $ Fit each of your models on the entire training set by calling the `.fit` method of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    # START ANSWER\n",
    "    model.fit(X, y)\n",
    "    # END ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "for model in models.values():\n",
    "    check_is_fitted(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluate\n",
    "The `sklearn.metrics` module has lots of metrics that can evaluate a model's predictions. Here is an example of how to calculate a model's F1 and accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB\n",
      "- accuracy_score 0.96\n",
      "- f1_score 0.96\n",
      "DummyClassifier\n",
      "- accuracy_score 0.3333333333333333\n",
      "- f1_score 0.5\n",
      "DecisionTreeClassifier\n",
      "- accuracy_score 0.98\n",
      "- f1_score 0.9800020002000202\n",
      "KNeighborsClassifier\n",
      "- accuracy_score 1.0\n",
      "- f1_score 1.0\n",
      "SVM\n",
      "- accuracy_score 0.9733333333333334\n",
      "- f1_score 0.973344004268374\n",
      "LogisticRegression\n",
      "- accuracy_score 0.9666666666666667\n",
      "- f1_score 0.9666700003333667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "for name, model in models.items():\n",
    "    prediction = model.predict(X)\n",
    "    f1_score_value = f1_score(prediction, y, average=\"weighted\")\n",
    "    accuracy = accuracy_score(prediction, y)\n",
    "    print(name)\n",
    "    print(\"- accuracy_score\", accuracy)\n",
    "    print(\"- f1_score\", f1_score_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data splitting\n",
    "Models usually achieve a high evaluation score on the training set. However, this doesn't say anything about how well it generalizes to unseen data. We usually evaluate models using either a test/validation split or k-fold validation. Scikit-learn  makes our life easier here by implementing both functions for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Test/validation split\n",
    "We can split datasets into training and test sets using the `train_test_split` function. The `test_size` parameter indicates the percentage of data that should go to the test set. The `stratify`  parameter indicate that the split should take the distribution of target labels `y` into account during the split. This parameter ensures that both the train and test have the same distribution of target variables.\n",
    "\n",
    "$ \\ex{3} $ The data has already been split into a training and a test set. Fit the model using the training set and evaluate them for `accuracy` and `weighted F1-score` using the test set.\n",
    "\n",
    "The result on the test set should roughly be equal to:\n",
    "\n",
    "|                  Model |    Accuracy  | F1 |\n",
    "|-----------------------:|------:|---------:|\n",
    "|             GaussianNB |  0.86 |     0.86 |\n",
    "| DummyClassifier        | 0.33  | 0.166      |\n",
    "| DecisionTreeClassifier | 0.866 |    0.866 |\n",
    "| KNeighborsClassifier   | 1     | 1        |\n",
    "| SVM                    | 0.93     | 0.932        |\n",
    "| LogisticRegression     | 0.933 | 0.932    |\n",
    "\n",
    "Manually verify that this is indeed the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB\n",
      "- accuracy_score 0.8666666666666667\n",
      "- f1_score 0.8666666666666667\n",
      "DummyClassifier\n",
      "- accuracy_score 0.3333333333333333\n",
      "- f1_score 0.16666666666666666\n",
      "DecisionTreeClassifier\n",
      "- accuracy_score 0.8666666666666667\n",
      "- f1_score 0.8666666666666667\n",
      "KNeighborsClassifier\n",
      "- accuracy_score 1.0\n",
      "- f1_score 1.0\n",
      "SVM\n",
      "- accuracy_score 0.9333333333333333\n",
      "- f1_score 0.9326599326599326\n",
      "LogisticRegression\n",
      "- accuracy_score 0.9333333333333333\n",
      "- f1_score 0.9326599326599326\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, shuffle=True, stratify=y)\n",
    "\n",
    "# START ANSWER\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    prediction = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, prediction)\n",
    "    f1_score_value = f1_score(y_test, prediction, average=\"weighted\")\n",
    "    print(name)\n",
    "    print(\"- accuracy_score\", accuracy)\n",
    "    print(\"- f1_score\", f1_score_value)\n",
    "# END ANSWER "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## K-fold validation\n",
    "Setting up k-fold validation is a bit more work but we can do it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def k_fold_fit_and_evaluate(X, y, model, scoring_method, n_splits=5):\n",
    "    # Define evaluation procedure\n",
    "    cv = KFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
    "    # Evaluate model\n",
    "    scores = cross_validate(model, X, y, scoring=scoring_method, cv=cv, n_jobs=-1)\n",
    "    \n",
    "       \n",
    "    return scores[\"test_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Note:** `cross_validate` expects a `scoring_method`. We can create a `scoring_method` using the `make_scorer` function from scikit-learn.\n",
    "\n",
    "$ \\ex{4} $ Use the example below to calculate the mean and std for both the F1 and the accuracy score. The `k_fold_fit_and_evaluate` method returns the resulting k-fold validation score from the provided `scoring_method`.\n",
    "\n",
    "Hint: use `np.mean` and `np.std`.\n",
    "\n",
    "The result using k-fold validation should roughly be equal to:\n",
    "\n",
    "\n",
    "|                  Model | mean F1 | std F1 | mean Accuracy | std Accuracy |\n",
    "|-----------------------:|--------:|--------|--------------:|--------------|\n",
    "|             GaussianNB |   0.960 | 0.0249 |        0.960 | 0.0249        |\n",
    "| DummyClassifier        |0.1079   | 0.01866 | 0.26        | 0.0249       |\n",
    "| DecisionTreeClassifier |   0.946 | 0.0340 |       0.946 | 0.0339       |\n",
    "| KNeighborsClassifier   | 0.966   | 0.0214 | 0.966        | 0.02144      |\n",
    "| SVM                    | 0.980   | 0.0163 | 0.980        | 0.0163      |\n",
    "| LogisticRegression     | 0.966   | 0.0298 | 0.966        | 0.0298     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB\n",
      "0.960083877475182 0.02496762328890136 0.9600000000000002 0.024944382578492935\n",
      "DummyClassifier\n",
      "0.4120800962906226 0.031237010908414645 0.26 0.024944382578492935\n",
      "DecisionTreeClassifier\n",
      "0.9467734439473571 0.034088683237891465 0.9466666666666667 0.03399346342395189\n",
      "KNeighborsClassifier\n",
      "0.9669716271573856 0.020723569597320978 0.9666666666666668 0.02108185106778919\n",
      "SVM\n",
      "0.9800214654282765 0.0163127186195678 0.9800000000000001 0.01632993161855452\n",
      "LogisticRegression\n",
      "0.9666666666666668 0.029814239699997188 0.9666666666666668 0.029814239699997188\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "import numpy as np\n",
    "\n",
    "n_splits = 5\n",
    "\n",
    "\n",
    "scoring_method_f1 = make_scorer(lambda prediction, true_target: f1_score(true_target, prediction, average=\"weighted\"))\n",
    "# START ANSWER\n",
    "scoring_method_accuracy = make_scorer(lambda prediction, true_target: accuracy_score(true_target, prediction))\n",
    "# END ANSWER \n",
    "\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(name)\n",
    "    metrics_f1 = k_fold_fit_and_evaluate(X, y, model, scoring_method_f1, n_splits=n_splits) \n",
    "    # START ANSWER\n",
    "    metrics_accuracy = k_fold_fit_and_evaluate(X, y, model, scoring_method_accuracy, n_splits=n_splits)\n",
    "    print(np.mean(metrics_f1), np.std(metrics_f1), np.mean(metrics_accuracy), np.std(metrics_accuracy))\n",
    "    # END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Grid search\n",
    "Scikit-learn also makes it easier to tune hyper-parameters using `GridSearchCV`.\n",
    "\n",
    "$ \\ex{5} $ Extend the `model_parameters` dictionary by specifying a grid search for the `KNeighborsClassifier`, `SVM` and the `LogisticRegression` models. Choose a set of hyper-parameters values that seems reasonable for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB\n",
      "- best_score = 0.9599161225248183\n",
      "best paramters:\n",
      "DummyClassifier\n",
      "- best_score = 0.10791990370937739\n",
      "best paramters:\n",
      "DecisionTreeClassifier\n",
      "- best_score = 0.9465598893859765\n",
      "best paramters:\n",
      "- max_depth None\n",
      "- random_state 42\n",
      "KNeighborsClassifier\n",
      "- best_score = 0.9797720797720798\n",
      "best paramters:\n",
      "- n_neighbors 7\n",
      "- weights distance\n",
      "SVM\n",
      "- best_score = 0.9799785345717235\n",
      "best paramters:\n",
      "- C 5\n",
      "- gamma scale\n",
      "- kernel rbf\n",
      "LogisticRegression\n",
      "- best_score = 0.9666666666666668\n",
      "best paramters:\n",
      "- penalty l2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "random_state = 42\n",
    "n_splits = 5\n",
    "scoring_method = make_scorer(lambda true_target, prediction: f1_score(true_target, prediction, average=\"weighted\"))\n",
    "\n",
    "model_parameters = {\n",
    "    \"GaussianNB\": {\n",
    "    \n",
    "    },\n",
    "    \"DummyClassifier\": {\n",
    "        \n",
    "    },\n",
    "    \"DecisionTreeClassifier\": {\n",
    "        'random_state': [random_state],\n",
    "        'max_depth': [None, 2, 5, 10]\n",
    "    },\n",
    "    # START ANSWER\n",
    "    \"KNeighborsClassifier\" : {\n",
    "        \"n_neighbors\": [3, 5, 7, 9],\n",
    "        \"weights\": [\"uniform\", \"distance\"]\n",
    "    },\n",
    "    \"SVM\" : {\n",
    "        \"C\": [5, 10, 15],\n",
    "        \"kernel\": [\"poly\", \"linear\", \"rbf\"],\n",
    "        \"gamma\": [\"scale\", \"auto\"]\n",
    "    },\n",
    "    \"LogisticRegression\": {\n",
    "        \"penalty\": [\"none\", \"l2\"]\n",
    "    }\n",
    "    # END ANSWER\n",
    "}\n",
    "\n",
    "for model_name, parameters in model_parameters.items():\n",
    "    model = models[model_name]\n",
    "    \n",
    "    cv = KFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
    "    grid_search = GridSearchCV(model, parameters, cv=cv, n_jobs=-1, verbose=False, scoring=scoring_method).fit(X, y)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_score = grid_search.best_score_\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    print(model_name)\n",
    "    print(\"- best_score =\", best_score)\n",
    "    print(\"best paramters:\")\n",
    "    for k,v in best_params.items():\n",
    "        print(\"-\", k, v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Using Transformers\n",
    "The transformers have a similar but slightly different API than the models. Transformers still have the `fit` method. The fit method is, for example, used in the `StandardScaler` to find the `mean` and `std` values. However, the `predict` method is replaced with the `transform` method. Scikit-learn did this to make it clear to the users that this is not a model but a feature transformer.  \n",
    "**Note:** [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) can be used to center the features and scale them to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(array([5.82962963, 3.05703704, 3.75111111, 1.20518519]),\n array([0.82210877, 0.44297659, 1.74999965, 0.763842  ]))"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "scaler.mean_, scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After fitting the transformer, you can call the `transform` method. It will transform the input features based on the parameters found during the last `fit` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train\n",
      "mean 3.460740740740741\n",
      "std 1.9662465199534571\n",
      "\n",
      "X_train_transformed\n",
      "mean 6.579099405186112e-17\n",
      "std 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "print(\"X_train\")\n",
    "print(\"mean\", X_train.mean())\n",
    "print(\"std\", X_train.std())\n",
    "print()\n",
    "print(\"X_train_transformed\")\n",
    "print(\"mean\", X_train_transformed.mean())\n",
    "print(\"std\", X_train_transformed.std())\n",
    "\n",
    "assert np.isclose(X_train_transformed.mean(), 0)\n",
    "assert np.isclose(X_train_transformed.std(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "$ \\ex{6} $ First, transform the dataset using the `Normalizer` transformer. The fit and evaluate each model using the transformed features.  \n",
    "**Note:** [`Normalizer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer) scales the features of each sample so that it has unit norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 0.43844230430986214\n",
      "std 0.24035046451267422\n",
      "GaussianNB\n",
      "- accuracy_score 1.0\n",
      "- f1_score 1.0\n",
      "DummyClassifier\n",
      "- accuracy_score 0.3333333333333333\n",
      "- f1_score 0.16666666666666666\n",
      "DecisionTreeClassifier\n",
      "- accuracy_score 0.8\n",
      "- f1_score 0.7802197802197803\n",
      "KNeighborsClassifier\n",
      "- accuracy_score 1.0\n",
      "- f1_score 1.0\n",
      "SVM\n",
      "- accuracy_score 1.0\n",
      "- f1_score 1.0\n",
      "LogisticRegression\n",
      "- accuracy_score 0.8666666666666667\n",
      "- f1_score 0.861111111111111\n"
     ]
    }
   ],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, shuffle=True, stratify=y)\n",
    "\n",
    "scaler = preprocessing.Normalizer()\n",
    "\n",
    "# START ANSWER\n",
    "scaler = scaler.fit(X)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    prediction = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, prediction)\n",
    "    f1_score_value = f1_score(y_test, prediction, average=\"weighted\")\n",
    "    print(name)\n",
    "    print(\"- accuracy_score\", accuracy)\n",
    "    print(\"- f1_score\", f1_score_value)\n",
    "\n",
    "# END ANSWER"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}